{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Neural Networks: Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivation: Non-linear hypotheses\n",
    "\n",
    "Suppose you have a housing classification problem with 100 original features. Combinations of these features would likely add up to a very large number of total features (for logistic regression, it is roughly $\\frac{n^2}{2}$).\n",
    "\n",
    "It turns out, this is indeed both time consuming and very computationally expensive with our existing techniques.\n",
    "\n",
    "Using the example of **computer vision**, to recognize a car we could use the intensity of a 50x50 grid of pixels (2500 features - 7500 if RGB) or use *only* the quadratic features with our existing methods (around 3 **million** features).  \n",
    "The first approach looks more pragmatic (and a bit magic), so this section will cover how it works.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neurons and the brain\n",
    "\n",
    "Neural networks are algorithms which try to mimic the brain. Popular in 80s and early 90s, they are now back in town and are state-of-the-art for several applications (some of them covered later in this section).  \n",
    "\n",
    "Hypothesis behind the \"biological inspiration\": the one learning algorithm. In short, it means that physical hardware (brain tissue) does not determine what is learned. In experimental settings, it has been proven that e.g. the auditory cortex can _learn_ to see. \n",
    "Therefore, there must be an underlying algorithm that allows these different parts of the brain to learn to perform the same function.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Representation: Intuition\n",
    "\n",
    "On a (very very) simplicistic level, a neuron has three parts: the _dendrite_ (input), the _nucleus_ (computation) and the _axon_ (output). \n",
    "\n",
    "Our neuron model will be modeled as a **logistic unit**, therefore our previous function $g(z)$ can be referred to as **sigmoid (logistic) activation function**. \n",
    "\n",
    "$g(z) = \\frac{1}{1+e^{-z}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters we previously referred as theta are also called **weights**, especially in the literature. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A neural network is made of _at least_ 3 different types _layers_:\n",
    "\n",
    "1. **Input** layer\n",
    "2. **Hidden** layer\n",
    "3. **Output** layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Visual Representation of a Neural Network](Images/Neural_Network.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More precise denominations:\n",
    "\n",
    "$a_i^{(j)}$ = \"activation\" of unit $i$ in layer $j$  \n",
    "$\\Theta^{(j)}$ = matrix of weights controlling function mapping from layer $j$ to layer $j + 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formally:\n",
    "    \n",
    "$a_1^{{(2)}} = g(\\Theta_{10}^{(1)}x_0 + \\Theta_{11}^{(1)}x_1 + \\Theta_{12}^{(1)}x_2 + \\Theta_{13}^{(1)}x_3)$ \n",
    "\n",
    "$a_2^{{(2)}} = g(\\Theta_{20}^{(1)}x_0 + \\Theta_{21}^{(1)}x_1 + \\Theta_{22}^{(1)}x_2 + \\Theta_{23}^{(1)}x_3)$   \n",
    "\n",
    "$a_3^{{(2)}} = g(\\Theta_{30}^{(1)}x_0 + \\Theta_{31}^{(1)}x_1 + \\Theta_{32}^{(1)}x_2 + \\Theta_{33}^{(1)}x_3)$   \n",
    "\n",
    "$h_\\Theta(x) = a_1^{{(3)}} = g(\\Theta_{10}^{(2}a_0^{{(2)}} + \\Theta_{11}^{(2}a_1^{{(2)}} + \\Theta_{12}^{(2}a_2^{{(2)}} + \\Theta_{13}^{(3}a_0^{{(2)}})$    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of dimensions, we can always expect $\\theta^{(j)}$ to have dimension $s_{j+1} x s_j + 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The approach we just used is called **forward propagation**. To calculate it more efficiently, we are going to use a vectorized implementation.\n",
    "\n",
    "But first, let's simplify the above as:\n",
    "\n",
    "$a_1^{{(2)}} = g(z_1^{{(2)}})$   \n",
    "\n",
    "$a_2^{{(2)}} = g(z_2^{{(2)}})$   \n",
    "\n",
    "$a_3^{{(2)}} = g(z_3^{{(2)}})$   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at our coefficients and variables, we can see how they can both be representated by vectors. \n",
    "The computation is now much easier:\n",
    "\n",
    "$z^{(2)} = \\Theta^{(1)}x$  \n",
    "\n",
    "$a^{(2)} = g(z^{(2)})$\n",
    "\n",
    "with their matrix multiplication being equal to the $\\Theta$ matrix described above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To these, we will need to add $a_0^{(2)} = 1$\n",
    "\n",
    "Finally:\n",
    "\n",
    "$z^{(3)} = \\Theta^{(2)}a^{(2)}$  \n",
    "\n",
    "$h_\\Theta(x) = a^{(3)} = g(z^{(3)})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, what neural network is doing is similar to what happens with logistic regression, **but** instead of using our original features, it uses the hidden layer features / activators. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
