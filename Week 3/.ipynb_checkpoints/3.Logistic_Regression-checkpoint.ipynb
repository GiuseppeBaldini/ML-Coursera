{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "In a classification problem, the variable we are trying to classify has two possible values: 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ y \\in \\{0,1\\}$  with 0 = **Negative class** | 1 = **Positive class**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: In a multiclass classification problem, our variable can assume multiple values.  \n",
    "\n",
    "In our simplest case - **binary classification problem** - linear regression can assume value between 0 and 1: \n",
    "\n",
    "$ 0 \\le h_\\theta(x) \\le 1$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis Representation\n",
    "\n",
    "$ h_\\theta(x) = g(\\theta^T x) $  \n",
    "\n",
    "where g(z) is the **sigmoid function** or **logistic function**:\n",
    "\n",
    "$g(z) = \\frac{1}{1+e^{-z}}  =  \\frac{1}{1+e^{-\\theta^T x}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "$h_\\theta(x)$ = estimated probability that y = 1 on input x  \n",
    "$h_\\theta(x) = p (y=1 | x=0) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we predict:  \n",
    "\n",
    "$h_\\theta(x) \\ge 0.5 \\to y = 1$  \n",
    "\n",
    "$h_\\theta(x) < 0.5 \\to y = 0$  \n",
    "\n",
    "By design of our logistic function:\n",
    "\n",
    "$z \\ge 0 \\to g(z) \\ge 0.5 $\n",
    "\n",
    "Therefore:\n",
    "\n",
    "$\\theta^Tx \\ge 0 \\to  h_{\\theta}(x) = g(\\theta^Tx) \\ge 0.5 $  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **decision boundary** is the line which delimits the area where y = 0 or y = 1.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recap: Linear Regression**\n",
    "\n",
    "$Cost(h_{\\theta}(x),y) = \\frac{1}{2}(h_{\\theta}(x) - y)^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression**\n",
    "\n",
    "$Cost(h_{\\theta}(x),y) =\n",
    "\\begin{cases}\n",
    "-log(h_{\\theta}(x)) & y = 1 \\\\\n",
    "-log(1-h_{\\theta}(x)) & y = 0\n",
    "\\end{cases}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: we cannot use the linear regression function since it would not create a _convex_ function, creating multiple local optima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplified Cost Function\n",
    "\n",
    "$Cost(h_{\\theta}(x), y) = -ylog(h_{\\theta}(x)) - (1-y)log(1-h_{\\theta}(x))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ J(\\theta) = -\\frac{1}{m} [\\sum_{i=1}^m y^{(i)} log h_{\\theta}x^{(i)} + (1-y^{(i)}) log (1- h_{\\theta}x^{(i)})]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fit parameters $\\theta$:\n",
    "\n",
    "**min** $ J(\\theta)$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "\n",
    "Repeat {\n",
    "\n",
    "$\\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} J(\\theta) = \\theta_j - \\alpha \\sum_{i=1}^m (h_\\theta (x^{(i)}) - y^{(i)}) x_j^{(i)} $ \n",
    "\n",
    "_simultaneously update all $\\theta_j$_\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: this gradient descent looks exactly like the one we had for linear regression, but keep in mind that the hypothesis function $h_\\theta$ has changed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Optimization\n",
    "\n",
    "Turns out that gradient descent is only one of many optimization algorithms:\n",
    "* Conjugate gradient\n",
    "* BFGS \n",
    "* L-BFGS\n",
    "\n",
    "**Advantages**:\n",
    "\n",
    "1. No need to pick $\\alpha$ manually\n",
    "2. Faster than GD\n",
    "\n",
    "**Disadvantages**:\n",
    "\n",
    "1. Complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One vs. all classification\n",
    "\n",
    "To solve multiclassification, we can split the problem in individual binary classification problems. \n",
    "\n",
    "We will therefore need $i$ classifiers with $i$ = number of intended classes for our dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
