{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Machine Learning System Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we design to use our regularized linear regression algorithm to predict housing prices. \n",
    "\n",
    "After testing our hypothesis on a new set of housing data, the prediction errors become extremely large. What to do next?\n",
    "\n",
    "* Get more training examples\n",
    "* Try a smaller set of features (maybe our model is overfitting)\n",
    "* Try a bigger set of features (which includes additional data collection)\n",
    "* Try adding polynomial features ï¼ˆ$x_1^2, x_2^2, x_1x_2, etc.$) \n",
    "* Decreasing $\\lambda$\n",
    "* Increasing $\\lambda$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help us make better decisions, we can use what we will call **Machine Learning Diagnostic** , a test to run to get insights in what is / is not working with a learning algorithm, and therefore gain guidance on how to improve its performance.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating a Hypothesis\n",
    "\n",
    "How to tell if our hypothesis may be overfitting? We can use our own dataset for testing!\n",
    "\n",
    "A good rule of thumb ratio is training : testing is 70:30.   \n",
    "**Note**: Pay attention to whether the dataset is sorted (or has any inherent order) or not. \n",
    "\n",
    "In addition to the test set error for linear regression, we may also use the **misclassification error** for classification problems. \n",
    "In this case, the test error is:\n",
    "\n",
    "$ \\frac{1}{m_{test}} \\sum_{i=1}^{m_{test}} err(h_{\\theta}(x^{(i)}_{test}), y^{(i)}_{test})$\n",
    "\n",
    "where the $err$ function is a piecewise function which evaluates to 1 if the classification is **wrong** and 0 otherwise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection\n",
    "\n",
    "One way to break down our dataset into the three sets is:\n",
    "\n",
    "* Training set: **60%**\n",
    "* Cross validation set: **20%**\n",
    "* Test set: **20%**\n",
    "\n",
    "We can now calculate three separate error values for the three different sets using the following method:\n",
    "\n",
    "* Optimize the parameters in $\\Theta$ using the training set for each polynomial degree.\n",
    "* Find the polynomial degree d with the least error using the cross validation set.\n",
    "* Estimate the generalization error using the test set with $J_{test}(\\Theta^{(d)})$, (d = theta from polynomial with lower error);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias vs. Variance\n",
    "\n",
    "Suboptimal models can be reduced in the two classes: high bias (_= underfitting_) or high variance (_= overfitting_).\n",
    "\n",
    "As we can find out by visualizing our error in function of the degree of polynomial $d$, our error: \n",
    "\n",
    "* Descreases as we increase d in the training set\n",
    "* Usually assumes a **U-shape**, underfitting at low d (high error / high bias) and overfitting at high d (high error again / high variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prioritizing what to work on\n",
    "\n",
    "**Example: SPAM Classifier** \n",
    "\n",
    "Feature selection: use the training set to identify the most common 10,000 / 50,000 words and represent each email as vector of 1/0 if such a word is present (1) or not (0).\n",
    "\n",
    "**Question: How to optimally spend our time?**\n",
    "\n",
    "Options:\n",
    "\n",
    "* Collect more data\n",
    "* Develop sophisticated features\n",
    "* Develop algorithms to process your input in different ways (e.g. watch / Watch / watches / w4tch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Analysis \n",
    "\n",
    "Recommended approach for ML project:\n",
    "\n",
    "1. Start with simple algorithm\n",
    "2. Implement it and test in on cross-validation data\n",
    "3. Plot learning curve to decide next step\n",
    "4. Error analysis to understand if there are patterns in the errors\n",
    "\n",
    "Let's assume that our spam classifier has a very high error rate on the cross-validation set. What we could do is:\n",
    "\n",
    "* Check what type of email is it;\n",
    "* List what type of features would have helped making a more accurate classification\n",
    "\n",
    "In this regard, **numerical evaluation** (having a single number to understand if we are moving in the right direction) is extremely useful. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Skewed Data\n",
    "\n",
    "A particular case when it is difficult to have evaluation metrics is that of _skewed classes_.\n",
    "\n",
    "Simply speaking, a case where we are trying to classify something which occurs in a very small portion of the cases (e.g. 0.5% cancer diagnosis). \n",
    "\n",
    " _ | Actual 1 | Actual 0\n",
    "------------ | ------------- | ----\n",
    "Predicted 1 | True positive | False positive\n",
    "Predicted 0 | False negative | True negative\n",
    "\n",
    "Let's now introduce more sophisticated measures of evaluation: **precision** and **recall**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define **precision** as:  $ \\frac {\\text {True positive}}{\\text{True positive + False positive}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define **recall** as:   $ \\frac {\\text {True positive}}{\\text{True positive + False negative}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that there is intrinsic trade-off between precision and recall as our threshold varies. \n",
    "The next big question is therefore: is there a way to choose our threshold (cut-off value) automatically? \n",
    "\n",
    "It turns out there is, and it is called the $F_1$ Score. \n",
    "\n",
    "$F_1 \\text{ Score} = 2 \\frac{PR}{P+R}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data for Machine Learning\n",
    "\n",
    "In a famous paper from Banko and Brill (2001), the authors compare the performance of 4 different algorithms are the size of the dataset increases. In all of the cases, the performance improves, ending up almost converging for 3 of the them. \n",
    "\n",
    "\n",
    "**Large data rationale** \n",
    "1. An algorithm with many parameters will have a low training error\n",
    "2. Using a large dataset will make sure that training set error and test set error will be close to each other\n",
    "\n",
    "These two conditions should ensure that the test set error will end up being low.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
