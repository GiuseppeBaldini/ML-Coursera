{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks: Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will learn how to fit the parameters of the neural network given a training set. \n",
    "\n",
    "In our (classification) problems, we will be dealing with either:\n",
    "\n",
    "1. Binary classification (1 output unit)\n",
    "2. Multi-class classification (k output units)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function\n",
    "\n",
    "Our neural network cost function is a generalization of the logistic regression cost function:\n",
    "\n",
    "$J(\\Theta) = -\\frac{1}{m}[\\sum_{i=1}^m \\sum_{k=1}^K y_k^{(i)} log(h_\\Theta(x^{(i)}))_k + (1-y_k^{(i)})log(1-(h_\\Theta(x^{(i)}))_k)] + \\frac{\\lambda}{2m} \\sum_{l=1}^{L-1} \\sum_{i=1}^{s_l} \\sum_{j=1}^{s_{l+1}} (\\Theta_{ji}^{(l)})^2 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes**:\n",
    "\n",
    "1. The double sum adds up the logistic regression costs calculated for each cell in the output layer;\n",
    "2. The triple sum adds up the squares of all the individual $\\Theta$s in the entire network;\n",
    "3. the i in the triple sum does **not** refer to training example i;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation algorithm\n",
    "\n",
    "Just as a remainder, what we did in the previous section (Week 4) was **forward propagation**. Starting from the first layer all the way to the output layer.\n",
    "\n",
    "**Intuition**: for each node $j$ in layer $l$ we will calculate the _error_ $\\delta_j^{(l)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The backpropagation algorithm works as follows:\n",
    "\n",
    "Given a training set $\\{ (x^{(1)}, x^{(2)} ) \\cdots (x^{(m)}, y^{(m)}) \\}$\n",
    "\n",
    "1. $\\Delta^{(l)}_ij = 0$ for all $(l,i,j)$\n",
    "\n",
    "2. Perform forward propagation to compute $a^{(l)}$\n",
    "\n",
    "3. Using $y^{(t)}$, compute $\\delta^{(L)} = a^{(L)} âˆ’ y^{(t)}$  \n",
    "L = last layer\n",
    "\n",
    "4. Compute $\\delta$ **backwards** starting from L-1:  \n",
    "$\\delta^{(l)} = ((\\Theta^{(l)})^T \\delta^{(l+1)}) .* a^{(l)} .* (1-a^{(l)})$  \n",
    "$g'(z^{(l)}) = a^{(l)} .* (1-a^{(l)})$  \n",
    "\n",
    "5. $\\Delta^{(l)}_ij = \\Delta^{(l)}_ij + a^{(l)}_j \\delta^{(l+1)} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation Note: Unrolling Parameters\n",
    "\n",
    "In order to speed up computation, it can be beneficial unroll our initial parameters $\\Theta^{(1)}$, $\\Theta^{(2)}$ and $\\Theta^{(3)}$ in a **single vector** <code>initialTheta</code>.\n",
    "\n",
    "At this point, we apply:\n",
    "\n",
    "<code>fminunc(@costFunction, initialTheta, options)</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Checking \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
