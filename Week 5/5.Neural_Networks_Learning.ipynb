{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Neural Networks: Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will learn how to fit the parameters of the neural network given a training set. \n",
    "\n",
    "In our (classification) problems, we will be dealing with either:\n",
    "\n",
    "1. Binary classification (1 output unit)\n",
    "2. Multi-class classification (k output units)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function\n",
    "\n",
    "Our neural network cost function is a generalization of the logistic regression cost function:\n",
    "\n",
    "$J(\\Theta) = -\\frac{1}{m}[\\sum_{i=1}^m \\sum_{k=1}^K y_k^{(i)} log(h_\\Theta(x^{(i)}))_k + (1-y_k^{(i)})log(1-(h_\\Theta(x^{(i)}))_k)] + \\frac{\\lambda}{2m} \\sum_{l=1}^{L-1} \\sum_{i=1}^{s_l} \\sum_{j=1}^{s_{l+1}} (\\Theta_{ji}^{(l)})^2 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes**:\n",
    "\n",
    "1. The double sum adds up the logistic regression costs calculated for each cell in the output layer;\n",
    "2. The triple sum adds up the squares of all the individual $\\Theta$s in the entire network;\n",
    "3. the i in the triple sum does **not** refer to training example i;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation algorithm\n",
    "\n",
    "Just as a remainder, what we did in the previous section (Week 4) was **forward propagation**. Starting from the first layer all the way to the output layer.\n",
    "\n",
    "**Intuition**: for each node $j$ in layer $l$ we will calculate the _error_ $\\delta_j^{(l)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The backpropagation algorithm works as follows:\n",
    "\n",
    "Given a training set $\\{ (x^{(1)}, x^{(2)} ) \\cdots (x^{(m)}, y^{(m)}) \\}$\n",
    "\n",
    "1. $\\Delta^{(l)}_ij = 0$ for all $(l,i,j)$\n",
    "\n",
    "2. Perform forward propagation to compute $a^{(l)}$\n",
    "\n",
    "3. Using $y^{(t)}$, compute $\\delta^{(L)} = a^{(L)} âˆ’ y^{(t)}$  \n",
    "L = last layer\n",
    "\n",
    "4. Compute $\\delta$ **backwards** starting from L-1:  \n",
    "\n",
    "$\\delta^{(l)} = ((\\Theta^{(l)})^T \\delta^{(l+1)}) .* a^{(l)} .* (1-a^{(l)})$  \n",
    "$g'(z^{(l)}) = a^{(l)} .* (1-a^{(l)})$  \n",
    "\n",
    "5. $\\Delta^{(l)}_ij = \\Delta^{(l)}_ij + a^{(l)}_j \\delta^{(l+1)} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation Note: Unrolling Parameters\n",
    "\n",
    "In order to speed up computation, it can be beneficial unroll our initial parameters $\\Theta^{(1)}$, $\\Theta^{(2)}$ and $\\Theta^{(3)}$ in a **single vector** <code>initialTheta</code>.\n",
    "\n",
    "At this point, we apply:\n",
    "\n",
    "<code>fminunc(@costFunction, initialTheta, options)</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Checking \n",
    "\n",
    "We can use gradient checking to make sure that our backpropagation works as we intend to. We can approximate the deriv of our cost function to:\n",
    "\n",
    "$ \\frac{\\partial } {\\partial \\Theta} J(\\Theta) \\approx \\frac{J(\\Theta + \\epsilon) - J(\\Theta - \\epsilon)} { 2\\epsilon}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: make sure to use an $\\epsilon$ small enough (e.g. $10^{-4}$) but not too small to avoid running in numerical problems as $\\epsilon \\approx 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Initialization\n",
    "\n",
    "When we backpropagate, initializing weight to zero is not effective. Instead, it is helpful to initialize randomly in an interval of $[\\epsilon, -\\epsilon]$\n",
    "\n",
    "A useful algorithm to follow would be:  \n",
    "\n",
    "<code>Theta1 = rand(n,m) * (2 * INIT_EPSILON) - INIT_EPSILON;   \n",
    " Theta2 = rand(n,m) * (2 * INIT_EPSILON) - INIT_EPSILON;  \n",
    "Theta3 = rand(1,m) * (2 * INIT_EPSILON) - INIT_EPSILON;  \n",
    "</code>\n",
    "\n",
    "**Note**: this $\\epsilon$ is NOT related to the one in Gradient Checking.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "Let's put it all together. \n",
    "\n",
    "First of all, we have to decide our network architecture, including:\n",
    "\n",
    "* Number of input units = dimension of features $x^{(i)}$\n",
    "* Number of output units = number of classes\n",
    "* Number of hidden units per layer = usually the more the better (must balance with cost of computation)\n",
    "\n",
    "Then we are ready to train our NN.\n",
    "\n",
    "1. Randomly initialize the weights\n",
    "2. Implement forward propagation to get $h \\Theta(x^{(i)})$ for any $x^{(i)}$\n",
    "3. Implement the cost function\n",
    "4. Implement backpropagation to compute partial derivatives\n",
    "5. Use gradient checking to confirm that your backpropagation works. Then disable gradient checking.\n",
    "6. Use gradient descent or a built-in optimization function to minimize the cost function with the weights in theta.\n",
    "\n",
    "**Note**: $J(\\Theta)$ is not convex and thus we can end up in a local minimum. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applications \n",
    "\n",
    "An interesting application is autonomous driving. In the example presented in the course video, a backpropagation algorithm can learn to \"steer\" from to the input provided by a human driver and the picture of the road acquired at regular intervals. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
