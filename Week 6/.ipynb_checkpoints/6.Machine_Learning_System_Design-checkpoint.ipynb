{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Machine Learning System Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we design to use our regularized linear regression algorithm to predict housing prices. \n",
    "\n",
    "After testing our hypothesis on a new set of housing data, the prediction errors become extremely large. What to do next?\n",
    "\n",
    "* Get more training examples\n",
    "* Try a smaller set of features (maybe our model is overfitting)\n",
    "* Try a bigger set of features (which includes additional data collection)\n",
    "* Try adding polynomial features ï¼ˆ$x_1^2, x_2^2, x_1x_2, etc.$) \n",
    "* Decreasing $\\lambda$\n",
    "* Increasing $\\lambda$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help us make better decisions, we can use what we will call **Machine Learning Diagnostic** , a test to run to get insights in what is / is not working with a learning algorithm, and therefore gain guidance on how to improve its performance.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating a Hypothesis\n",
    "\n",
    "How to tell if our hypothesis may be overfitting? We can use our own dataset for testing!\n",
    "\n",
    "A good rule of thumb ratio is training : testing is 70:30.   \n",
    "**Note**: Pay attention to whether the dataset is sorted (or has any inherent order) or not. \n",
    "\n",
    "In addition to the test set error for linear regression, we may also use the **misclassification error** for classification problems. \n",
    "In this case, the test error is:\n",
    "\n",
    "$ \\frac{1}{m_{test}} \\sum_{i=1}^{m_{test}} err(h_{\\theta}(x^{(i)}_{test}), y^{(i)}_{test})$\n",
    "\n",
    "where the $err$ function is a piecewise function which evaluates to 1 if the classification is **wrong** and 0 otherwise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection\n",
    "\n",
    "One way to break down our dataset into the three sets is:\n",
    "\n",
    "* Training set: **60%**\n",
    "* Cross validation set: **20%**\n",
    "* Test set: **20%**\n",
    "\n",
    "We can now calculate three separate error values for the three different sets using the following method:\n",
    "\n",
    "* Optimize the parameters in $\\Theta$ using the training set for each polynomial degree.\n",
    "* Find the polynomial degree d with the least error using the cross validation set.\n",
    "* Estimate the generalization error using the test set with $J_{test}(\\Theta^{(d)})$, (d = theta from polynomial with lower error);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias vs. Variance\n",
    "\n",
    "Suboptimal models can be reduced in the two classes: high bias (_= underfitting_) or high variance (_= overfitting_).\n",
    "\n",
    "As we can find out by visualizing our error in function of the degree of polynomial $d$, our error: \n",
    "\n",
    "* Descreases as we increase d in the training set\n",
    "* Usually assumes a **U-shape**, underfitting at low d (high error / high bias) and overfitting at high d (high error again / high variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prioritizing what to work on\n",
    "\n",
    "**Example: SPAM Classifier** \n",
    "\n",
    "Feature selection: use the training set to identify the most common 10,000 / 50,000 words and represent each email as vector of 1/0 if such a word is present (1) or not (0).\n",
    "\n",
    "**Question: How to optimally spend our time?**\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
