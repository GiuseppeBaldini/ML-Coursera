{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Multivariate Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will learn about a more powerful linear regression, one that can deal with multiple variables (or features) $x^{i}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Notation\n",
    "\n",
    "$n$ = number of variables / features  \n",
    "$x^{(i)}$ = input / features of $i^{th}$ training sample  \n",
    "$x^{(i)}_j$ = value of feature $j$ in $i^{th}$ training sample  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore in this notation, $x^{(i)}$ is an $n$-dimensional **vector** (as above, with $n$ being the number of features / variables). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case our hypothesis function will look like:\n",
    "    \n",
    "$h_{\\theta}(x) = \\theta_0 + \\theta_1x_1 + \\theta_2x_2 + \\cdots + \\theta_nx_n $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, we will define $x_0 = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our vectors will therefore look like:\n",
    "    \n",
    "$x = \\left[\n",
    "\\begin{array}\n",
    "x_0 \\\\\n",
    "x_1 \\\\\n",
    "\\cdots \\\\\n",
    "x_n\n",
    "\\end{array}\n",
    "\\right]$\n",
    "\n",
    "$\\theta = \\left[\n",
    "\\begin{array}\n",
    "\\theta_0 \\\\\n",
    "\\theta_1 \\\\\n",
    "\\cdots \\\\\n",
    "\\theta_n\n",
    "\\end{array}\n",
    "\\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting them together, we can now write our hypothesis function as:\n",
    "\n",
    "$h_{\\theta}(x) = \\theta^T_x $ with $\\theta^T_x $ being the matrix product of vectors $x$ and $\\theta$ as defined above. This is a **vectorization** of our hypothesis function for one training example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Gradient Descent for multiple variables "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Repeat until convergence:_  \n",
    "$ {\n",
    "\\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_J}(\\theta_0, \\ldots, \\theta_n)} $  \n",
    "\n",
    "Simultaneously update for every $j$\n",
    "\n",
    "---\n",
    "\n",
    "_Repeat until convergence:_\n",
    "\n",
    "$ \\theta_0 := \\frac{\\partial}{\\partial \\theta_j} \\frac{1}{m} \\sum_{i=1}^m (h_{\\theta}(x^{(i)}) - y^{(i)})\\cdot x_0^{(i)} $ \n",
    "\n",
    "$ \\theta_1 := \\frac{\\partial}{\\partial \\theta_j} \\frac{1}{m} \\sum_{i=1}^m (h_{\\theta}(x^{(i)}) - y^{(i)}) \\cdot x_1^{(i)} $  \n",
    "\n",
    "$ \\theta_2 := \\frac{\\partial}{\\partial \\theta_j} \\frac{1}{m} \\sum_{i=1}^m (h_{\\theta}(x^{(i)}) - y^{(i)}) \\cdot x_2^{(i)} $  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Feature Scaling \n",
    "\n",
    "Idea: if the features are on a similar scale (within the same range), the gradient descent will converge more quickly. \n",
    "\n",
    "Practically, we want to get every feature _approximately_ in a $ -1 \\le x \\le 1 $ range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean normalization\n",
    "\n",
    "We can replace $x_i$ with $\\frac{x_i - \\mu_i}{s_i}$ to normalize features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Learning Rate\n",
    "\n",
    "\"**Debugging**\" the gradient descent, or better yet, _ensuring it is working properly_, can be done in difference ways:\n",
    "\n",
    "1. By plotting $min(J(\\theta))$ as a function of the number of iterations. Ideally, we would have a monotonous smooth curve going lower and to the right.\n",
    "\n",
    "2. Create an automatic convergence test (e.g. declare convergence if $J(\\theta)$ decreases by less than $10^-3$ in one iteration.\n",
    "\n",
    "**Note**: It is mathematically proven that for an $\\alpha$ sufficiently small, the Gradient Descent should decrease for every iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Features and Polynomial Regression\n",
    "\n",
    "There are several options when it comes to choosing features in order to **modify the form of our hypothesis function**.. We can:\n",
    "\n",
    "1. (Of course) Use features we already have \n",
    "2. Create our own (e.g. for house prices: $Area = Frontage \\times Depth$  \n",
    "3. Manipulate features to have different models (e.g. a _quadratic_ or _cubic_ model)\n",
    "\n",
    "**Note**: bear in mind how the value of ranges changes after manipulation in order to normalize properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Computing Parameters Analytically - Normal Equation\n",
    "\n",
    "Normal equation allows us to solve for $\\theta$ analytically. \n",
    "\n",
    "**Intuition**\n",
    "\n",
    "* Find the hyphotesis function minimum: $ \\frac{d}{d \\theta} J(\\theta) = 0$ \n",
    "* Solve for $\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Equation**\n",
    "\n",
    "Assuming $X$ to be the feature matrix of dimensions $m \\times (n+1)$ and $y$ to be an $m$-dimensional vector of the actual values, our $\\theta$ will be:\n",
    "\n",
    "$\\theta = (X^T X)^{-1}X^Ty$\n",
    "\n",
    "Note: $m$ = number of training examples | $n$ = number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normal equation in Octave\n",
    "\n",
    "pinv(X' * X) * X' * y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Octave",
   "language": "octave",
   "name": "octave"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
