{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A powerful algorithm widely used (_at least at the time of the video_) in both industry and academia are Support Vector Machines (SVM). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start to understand SVM starting from the sigmoid activation function of logistic regression. \n",
    "\n",
    "$h_{\\theta}(x) = \\frac{1}{1+e^{-\\theta^T x}} $\n",
    "\n",
    "For a single example, our cost function will be: \n",
    "\n",
    "$Cost(h_{\\theta}(x), y) = -ylog(h_{\\theta}(x)) - (1-y)log(1-h_{\\theta}(x))$\n",
    "\n",
    "\n",
    "### Graphical intuition\n",
    "\n",
    "**Part 1**ï¼š $y = 1$\n",
    "\n",
    "$Cost(h_{\\theta}(x), y) = -log(h_{\\theta}(x)) = -log\\frac{1}{1+e^{-\\theta^T x}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will modify this function and _split_ it in two linear parts (flat for $z > 1$ and straight line otherwise). \n",
    "\n",
    "$Cost_1(z)$\n",
    "\n",
    "![SVM Graphical Intuition 1](Figures/SVM_Graph_1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 2**: $y = 0$\n",
    "\n",
    "$Cost(h_{\\theta}(x), y) = -log(1-h_{\\theta}(x)) = -log(1 - \\frac{1}{1+e^{-\\theta^T x}})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now _split_ it in a similar way, this time with a theshold placed at $z = -1$:\n",
    "\n",
    "$Cost_0(z)$\n",
    "\n",
    "![SVM Graphical Intuition 2](Figures/SVM_Graph_2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function\n",
    "\n",
    "$ min_{\\theta} [\\sum_{i=1}^m y^{(i)} cost_1{\\theta}x^{(i)} + (1-y^{(i)}) cost_0(1- h_{\\theta}x^{(i)})] + \\frac{1}{2} \\sum_{j=1}^n \\theta^2_j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notation change**: for log reg our cost function structure would look something like this:\n",
    "\n",
    "$ A + \\lambda B$ (lambda trying to regularize / _balance_ B)\n",
    "\n",
    "By convention, with SVM we will change to:\n",
    "\n",
    "$ C A + B$ \n",
    "\n",
    "In the end, it's just a different way to control the trade-off. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Large Margin Intuition\n",
    "\n",
    "For SVM:\n",
    "\n",
    "If $y = 1$ we want $\\theta^T x \\ge 1$ (not just $\\ge 0$)  \n",
    "If $y = 0$ we want $\\theta^T x \\le -1$ (not just $\\le 0$)\n",
    "\n",
    "This means that we have an extra **margin** compared to logistic regression, since our function it will require more extreme cases to classify a value as 1 or 0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernels \n",
    "\n",
    "We know that for higher order polynomials computation can quickly become an issue, so how can we simplify our features?\n",
    "\n",
    "One useful idea would be to compute new features based on predetermined points (in our example, _landmarks_).\n",
    "\n",
    "Example of similarity function (**kernel**):  \n",
    "\n",
    "$f_i = similarity(x, l^{(i)}) = exp(-\\frac{||x - l^{(i)}||^2}{2\\sigma^2})$ (**gaussian kernel**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have two extremes where we could end up, which determine the range of possible values for our kernel functions:\n",
    "\n",
    "1. If $x \\approx l^{(i)}$: $f_i \\approx 1$\n",
    "2. If $x$ far from $l^{(i)}$: $f_i \\approx 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But how do we choose and use kernels? \n",
    "\n",
    "1. We are going to choose our first landmarks $l^{(i)}$ exactly as the first training example    \n",
    "2. We can now create a _feature vector_ $f$ which maps x to $l^{(i)}$.\n",
    "3. Now, we predict $y = 1$ if $\\theta^T f \\ge 0$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM parameters \n",
    "\n",
    "C = $\\frac{1}{\\lambda}$\n",
    "\n",
    "* Large $C$ = lower bias - high variance (small $\\lambda$ - may lead to overfitting)\n",
    "* Small $C$ = higher bias - low variance (large $\\lambda$ - may lead to underfitting) \n",
    "\n",
    "$\\sigma^2$\n",
    "\n",
    "* Large $\\sigma^2$: features $f_i$ vary more smoothly. Higher bias, lower variance. \n",
    "* Small $\\sigma^2$: features $f_i$ vary less smoothly. Lower bias, higher variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "Although it is not recommended to code the minimization function (as there are libraries which can do it more efficiently), we still have to specify:\n",
    "\n",
    "* Parameter C\n",
    "* Kernel:\n",
    "    - **No** kernel > linear kernel (basically returns a linear classifier)\n",
    "    - Gaussian kernel > need to choose $\\sigma^2$\n",
    "    - Other kernel (which needs to satisfy the \"Mercer's Theorem\" to make SVM packages' optimizations work properly)\n",
    "    \n",
    "**Note**: Perform feature scaling before using the Gaussian kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression vs. SVM\n",
    "\n",
    "Let $n$ = number of features and $m$ = number of training examples, it is reasonable to:\n",
    "\n",
    "1. If $n > m$ (e.g. text classification problem) > **logistic regression**\n",
    "2. If $n$ is small, $m$ is intermediate > **SVM** with Gaussian kernel \n",
    "3. If $n < m$  create/add features > use logistic regression or linear kernel"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
