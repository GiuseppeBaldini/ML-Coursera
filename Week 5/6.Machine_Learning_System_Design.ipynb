{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Machine Learning System Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we design to use our regularized linear regression algorithm to predict housing prices. \n",
    "\n",
    "After testing our hypothesis on a new set of housing data, the prediction errors become extremely large. What to do next?\n",
    "\n",
    "* Get more training examples\n",
    "* Try a smaller set of features (maybe our model is overfitting)\n",
    "* Try a bigger set of features (which includes additional data collection)\n",
    "* Try adding polynomial features ï¼ˆ$x_1^2, x_2^2, x_1x_2, etc.$) \n",
    "* Decreasing $\\lambda$\n",
    "* Increasing $\\lambda$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help us make better decisions, we can use what we will call **Machine Learning Diagnostic** , a test to run to get insights in what is / is not working with a learning algorithm, and therefore gain guidance on how to improve its performance.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating a Hypothesis\n",
    "\n",
    "How to tell if our hypothesis may be overfitting? We can use our own dataset for testing!\n",
    "\n",
    "A good rule of thumb ratio is training : testing is 70:30.   \n",
    "**Note**: Pay attention to whether the dataset is sorted (or has any inherent order) or not. \n",
    "\n",
    "In addition to the test set error for linear regression, we may also use the **misclassification error** for classification problems. \n",
    "In this case, the test error is:\n",
    "\n",
    "$ \\frac{1}{m_{test}} \\sum_{i=1}^{m_{test}} err(h_{\\theta}(x^{(i)}_{test}), y^{(i)}_{test})$\n",
    "\n",
    "where the $err$ function is a piecewise function which evaluates to 1 if the classification is **wrong** and 0 otherwise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection\n",
    "\n",
    "One way to break down our dataset into the three sets is:\n",
    "\n",
    "* Training set: **60%**\n",
    "* Cross validation set: **20%**\n",
    "* Test set: **20%**\n",
    "\n",
    "We can now calculate three separate error values for the three different sets using the following method:\n",
    "\n",
    "* Optimize the parameters in $\\Theta$ using the training set for each polynomial degree.\n",
    "* Find the polynomial degree d with the least error using the cross validation set.\n",
    "* Estimate the generalization error using the test set with $J_{test}(\\Theta^{(d)})$, (d = theta from polynomial with lower error);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias vs. Variance\n",
    "\n",
    "Suboptimal models can be reduced in the two classes: high bias (_= underfitting_) or high variance (_= overfitting_).\n",
    "\n",
    "As we can find out by visualizing our error in function of the degree of polynomial $d$, our error: \n",
    "\n",
    "* Descreases as we increase d in the training set\n",
    "* Usually assumes a **U-shape**, underfitting at low d (high error / high bias) and overfitting at high d (high error again / high variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Errors as function of degree of polynomial](Figures/Errors_Degs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization and Bias / Variance\n",
    "\n",
    "The regularization term is a useful tool to contain overfitting. Before, it was given. Now, we have the intuition to understand how to choose it more rigorously. \n",
    "\n",
    "Practically, we can choose $\\lambda$ as follows:\n",
    "\n",
    "1. Take 12 models ($\\lambda = 0, 0.01, 0.02, 0.04, .. 10$)\n",
    "2. Minimize their $J(\\Theta)$ \n",
    "3. Use the $\\Theta$ found in step 2 to find $J_{cv}(\\Theta)$ **without** regularization\n",
    "4. Choose the $\\lambda$ with the lowest $J_{cv}(\\Theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Curves \n",
    "\n",
    "We can plot our learning curve as the graph that plots errors as a function of number of training set size:\n",
    "\n",
    "* Training error will increase with size\n",
    "* Cross validation error will decrease with size \n",
    "\n",
    "For high bias (underfitting) it doesn't matter if we add more training samples, we hit a limit > More data doesn't help. \n",
    "\n",
    "For high variance (overfitting) > More training data is helpful to better characterize the model\n",
    "\n",
    "Error | High Bias | High Variance\n",
    "------------ | ------------- | ----\n",
    "Training | Increase > flat out | Slowly increase\n",
    "Cross validation | Decrease > flat out | Slowly decrease\n",
    "Convergence | Yes | No"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
