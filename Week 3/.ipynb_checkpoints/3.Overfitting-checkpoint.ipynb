{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting\n",
    "\n",
    "If our algorithm does not capture enough variance of the data, we are going towards _underfitting_ or _high bias_.\n",
    "\n",
    "On the other hand, if our algorithm is following the dataset beyond the underlying \"rule\" that generated it (therefore confusing the specificity of the data with the general rule) we are going to have an **overfitting** or **high variance** problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Addressing Overfitting**\n",
    "\n",
    "1. Reduce the number of features\n",
    "        - Manually select features\n",
    "        - Model selection algorithm\n",
    "\n",
    "2. Regularization \n",
    "        - Keeps all features, but reduces magnitude of parameters $\\theta_j$\n",
    "        - Works well with a lot of features, each of which contributes to predicting $y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regularization and Cost Function**\n",
    "\n",
    "Intuition: if we have smaller values for parameters, this will lead to simpler hypothesis and less model overfitting. \n",
    "\n",
    "We can do this by adding a **regularization term** to our default equation: \n",
    "\n",
    "$ J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^m (h_\\theta (x^{(i)}) - y^{(i)})^2 + \\lambda \\sum_{i=1}^n \\theta_j^2 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized Linear Regression\n",
    "\n",
    "**Gradient Descent**\n",
    "\n",
    "Repeat {\n",
    "\n",
    "$\\theta_j := \\theta_j (1 - \\alpha \\frac{\\lambda}{m}) - \\alpha \\frac{1}{m} \\sum_{i=1}^m (h_\\theta (x^{(i)}) - y^{(i)}) x_j^{(i)} $ \n",
    "\n",
    "_simultaneously update all $\\theta_j$_\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normal Equation**\n",
    "\n",
    "To our normal equation we will need to add the term:  \n",
    "\n",
    "$\\lambda$ x $\\left[\n",
    "\\begin{array}{ccc}\n",
    "0 & 0 & 0 \\\\\n",
    "0 & 1 & 0\\\\\n",
    "0 & 0 & 1\n",
    "\\end{array}\n",
    "\\right]$  (example for 3 x 3 - always $M_{11} = 0$)\n",
    "\n",
    "**Note**: adding the regularization term also takes care of invertibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized Logistic Regression\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
