{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will start working with **unsupervised** algorithms, which work with unlabeled data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good example is **clustering**, where we look for _groups_ (clusters) in the data. Here is our first clustering algorithm: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means algorithm\n",
    "\n",
    "The K-means algorithm is by far the most popular and widely used (_at the time of the video_) clustering algorithm. \n",
    "\n",
    "Assuming I want to find $n$ clusters from my dataset, our algorithm will: \n",
    "\n",
    "1. Initialize two random $n$ points (**clusters centroids**)\n",
    "2. Assign all points to one of the $n$ clusters\n",
    "3. Move the centroids to the **average** of the new clusters\n",
    "4. Rinse and repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notation:\n",
    "\n",
    "* $K$ = number of clusters \n",
    "* $c^{(i)}$ = cluster centroid $i$\n",
    "* $x^{(i)}$ = center of cluster $i$\n",
    "* $\\mu_k$ = average of points assigned to cluster $k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization objective\n",
    "\n",
    "Like the previous algorithms we have seen, K-means has its own minimization function. Understanding it will be useful for two reasons:\n",
    "\n",
    "* Debugging / checking that everything runs properly\n",
    "* Avoiding to end up in local minima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the notation introduce above, our optimization objective is:\n",
    "\n",
    "$J(c^{(1)}, \\ldots, c^{(m)}, \\mu_1, \\ldots , \\mu_k) = \\frac{1}{m} \\sum_{i=1}^m ||x^{(i)} - \\mu_{c^{(i)}}||^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random initialization\n",
    "\n",
    "The most effective method is picking $K$ distinct random training examples and assign them as our first centroids. \n",
    "\n",
    "This works especially well if we initialize randomly multiple times. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the number of clusters\n",
    "\n",
    "Although choosing a number of clusters can feel somehow arbitrary, a method often implemented is the _elbow method_, which plots the variation of $J$ (cost function) as a function of $K$ (n. of clusters) and looks for an \"elbow\" (sudden variation in slope of the curve). \n",
    "\n",
    "Unfortunately, often the elbow is not that clear and can be hard to choose, making the method not particularly more helpful than chance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first motivation behind dimensionality reduction is **data compression**.\n",
    "\n",
    "In an example of 3D data, we can represent the points in our 3D coordinates by projecting them on a 2D plane. By doing so, we can represent almost the same data using two dimensions instead of three.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second is **visualization**, which can obviously be very challenging for datasets with large number of features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now introduce a dimensionality reduction algorithm: Principal Component Analysis (PCA).  \n",
    "PCA finds directions (vectors) onto which to project the data so as to minimize the projection error.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: PCA is different from LR. In PCA, we are minimizing the distance from the vector, while in LR we are miniming the error in predicting y from x.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA algorithm (in words):\n",
    "\n",
    "1. Preprocessing (feature scaling / mean normalization)   \n",
    "1A. Mean = $\\mu_j = \\frac{1}{m} \\sum_{i=1}^m x_j^{(i)}$  \n",
    "1B. Replace $x_j^{(i)}$ with $\\mu_j - x_j^{(i)}$\n",
    "1C. If different scales, we can standardize using standard deviation\n",
    "\n",
    "2. Compute covariance matrix  $ \\Sigma = \\frac{1}{m} \\sum_{i=1}^n (x^{(i)})(x^{(i)})^T$  \n",
    "Compute _eigenvectors_ of matrix $\\Sigma$ using svd (Single Value Decomposition)\n",
    "\n",
    "3. $\\text{U_reduce}$ = first $k$ (target dimension) vectors of matrix $U$ \n",
    "\n",
    "4. $z = \\text{U_reduce} ^T \\cdot x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA algorithm (in `Octave`):\n",
    "\n",
    "1. Preprocessing (feature scaling / mean normalization) \n",
    "1A. `mean = 1/m * sum(X)`  \n",
    "1B. `X = (X-mean) / st_dev`  \n",
    "\n",
    "2. `Sigma = (1/m) * X' * X;`  \n",
    "`[U, S, V] = svd(Sigma);`  \n",
    "\n",
    "3. `U_reduce = U(:, 1:k);` \n",
    "\n",
    "4. `z = U_reduce' * x;`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting from the compressed representation, we can go back to the original state (reconstruction) by:\n",
    "\n",
    "`x_approx = U_reduce * z`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choosing the number of principal components \n",
    "\n",
    "How do we decide to how many dimensions reduce our original dataset ($k$)?\n",
    "\n",
    "Generally, a good rile of thumb is choosing $k$ (n. of principal components) so that:\n",
    "\n",
    "$\\displaystyle \\frac{\\text {Average squared projection error}}{\\text {Total variation of the data}} = \\displaystyle \\frac {\\frac{1}{m} \\sum_{i=1}^m || x^{(i)}) - (x^{(i)}_{approx} || ^2} { \\frac{1}{m} \\sum_{i=1}^m || x^{(i)})||^2} \\le 0.01$ \n",
    "\n",
    "This ensures that _at least_ 99% of the variance of the data is retained. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** = instead of trying tentatively, we can use the parameter $S$ returned by the `Sigma` function above. \n",
    "\n",
    "It can proved in fact that $ \\frac {\\frac{1}{m} \\sum_{i=1}^m || x^{(i)}) - (x^{(i)}_{approx} || ^2} { \\frac{1}{m} \\sum_{i=1}^m || x^{(i)})||^2 } = 1 - \\frac{ \\sum ^k _{i=i} S_{ii}}{\\sum ^n _{i=i} S_{ii}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so we can simply choose what makes $ \\displaystyle \\frac{ \\sum ^k _{i=i} S_{ii}}{\\sum ^n _{i=i} S_{ii}} \\ge 0.99 $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advice for applying PCA\n",
    "\n",
    "A simple step-by-step implementation could be:\n",
    "\n",
    "1. Extract input from dataset (unlabeled dataset)  \n",
    "\n",
    "2. Reduce them using PCA  # only to training set\n",
    "\n",
    "3. Feed this new data to our classifier\n",
    "\n",
    "4. Test on test set "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
